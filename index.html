<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>RoboNurse-VLA</title>
  <link rel="stylesheet" href="styles.css">
</head>
<body>
  <header>
    <h1>RoboNurse-VLA</h1>
    <p>A Robotic Scrub Nurse System based on Vision-Language-Action Model</p>
  </header>

  <section id="about">
    <h2>About the Project</h2>
    <p>
      RoboNurse-VLA is a cutting-edge robotic scrub nurse system leveraging the power of vision-language-action models 
      to optimize instrument handover during surgeries. This system integrates the Segment Anything Model 2 (SAM 2) 
      with the Llama 2 language model for superior performance in grasping and handing over instruments, even for 
      challenging or unseen items.
    </p>
    <p>
      <strong>Authors:</strong> Shunlei Li, Jin Wang, Rui Dai, Wanyu Ma, Wing Yin Ng, Yingbai Hu, Zheng Li
    </p>
  </section>

  <section id="results">
    <h2>Experiments and Results</h2>
    <p>Results and evaluations of RoboNurse-VLA demonstrate significant improvements in performance compared to state-of-the-art models.</p>
    <ul>
      <li>Zero-shot performance: RoboNurse-VLA achieved a success rate of 65% in lift tasks.</li>
      <li>Fine-tuned models: 100% success rate on table-based instrument handover tasks.</li>
      <li>Unseen tools: 90% success rate for unseen instruments.</li>
      <li>Difficult-to-grasp items: 95% success rate with challenging objects.</li>
    </ul>
  </section>

  <footer>
    <p>More details can be found in the paper and GitHub repository.</p>
  </footer>
</body>
</html>
